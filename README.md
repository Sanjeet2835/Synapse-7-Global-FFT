# AI vs Real Image Detection using Global FFT

## Overview

This project implements a **binary classifier** to distinguish **AI-generated images** from **real (natural) images**, with a strong emphasis on **generator-agnostic detection**.

Rather than attempting to identify *which generator* produced an image, the model focuses on learning **frequency-domain patterns** that are characteristic of AI-generated content across multiple generators.

---

## Core Idea

AI-generated images contain subtle but consistent artifacts in the **frequency domain** that differentiate them from real images.
While each generator introduces its own unique frequency patterns, these patterns share common AI-specific characteristics.

By training on images generated by **multiple generators** and transforming images into the **frequency domain (global FFT magnitude)**, the model learns representations that generalize across generators.
This enables robust **AI vs Real** discrimination instead of generator-specific classification.

---

## Dataset

**Tiny GenImage (ImageNet-based)**

### AI Image Generators

* Wukong
* GLIDE
* BigGAN
* VQ-Diffusion (VQDM)
* Midjourney
* Stable Diffusion v5
* ADM (Diffusion Model)

### Real Images

* ImageNet (natural images)

> Generator labels are **not used as training targets**.
> They are retained only for **evaluation and analysis**.

---

## Problem Formulation

* **Task:** Binary classification
* **Classes:** `{AI, Real}`
* **Objective:** Learn generator-agnostic features for AI image detection

---

## Model Architecture

* **Backbone:** ResNet-34 (ImageNet pretrained)
* **Input:** RGB image
* **Feature Augmentation:** Global FFT magnitude representation
* **Output:** Binary prediction (AI vs Real)

The FFT features reduce over-reliance on spatial appearance and help the model capture generator-independent frequency artifacts.

---

## Training Strategy

Due to the relatively small size of the Tiny GenImage dataset, **only the final layers of ResNet-34 are fine-tuned**.

The early layers are kept frozen because they already capture generic visual primitives such as:

* edges
* textures
* gradients
* low-frequency structural patterns

Freezing these layers:

* Improves convergence speed
* Stabilizes gradients
* Reduces overfitting risk

Full end-to-end retraining is generally beneficial **only when the dataset is sufficiently large**.

---

## Data Transforms

Training in the frequency domain imposes additional constraints compared to RGB-domain training.

Many common spatial or color augmentations can **distort or destroy frequency signatures**, which are critical for FFT-based learning.
As a result:

* Only minimal and safe transforms are applied
* Aggressive augmentations are intentionally avoided

This preserves the integrity of frequency-domain information.

---

## Training Details

* **Loss Function:** Cross-Entropy Loss
* **Optimizer:** AdamW (with weight decay)
* **Learning Rate Scheduler:** ReduceLROnPlateau (monitored on validation loss)
* **Image Size:** 224 Ã— 224
* **Sampling Strategy:** Standard shuffled sampling
* **Generator Balancing:** Not applied during training

Generator information is **not used** in the training objective.

---

## Evaluation

Training and evaluation are intentionally **separated**.

Evaluation includes:

* Overall performance metrics
* Generator-wise analysis
* Threshold stress testing
* Calibration analysis

ðŸ‘‰ Full evaluation methodology, interpretation, and results are documented in
**`docs/evaluation.md`**

---

## Model Weights

Trained model weights are **not stored directly in this repository** due to GitHub file size limitations.

Download links and metadata are provided in:

```
models/README.md
```

---

## Repository Structure

```
ai-vs-real/
â”œâ”€â”€ README.md                  # Project overview & training details
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ resnet34-globalfft-v1-training.ipynb
â”‚   â””â”€â”€ resnet34-globalfft-v1-evaluation.ipynb
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ evaluation.md          # Detailed evaluation & analysis
â”œâ”€â”€ results/                   # Metrics, plots, reports
â”œâ”€â”€ models/
â”‚   â””â”€â”€ README.md              # External model links
â””â”€â”€ requirements.txt
```

---

## Reproducibility Notes

* Deterministic dataset splits
* Explicit generator mapping
* Fixed evaluation protocol
* Training and evaluation logic kept separate

---

## Scope & Limitations

* This is **not a face-only deepfake detector**
* Performance may degrade under:

  * Heavy compression
  * Aggressive resizing
  * Strong post-processing
* Continuous evaluation is required as new generators emerge

---

## Disclaimer

This project is intended for **research and educational purposes only** and should not be used as a standalone system in high-stakes applications.

